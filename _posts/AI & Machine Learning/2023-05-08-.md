决策树
信息熵

$$
E(S) = -\sum_{i=1}^{n}p_i\log_2p_i
$$
信息增益
含义：

$$
Gain(S,A) = E(S) - \sum_{v\in Values(A)}\frac{|S_v|}{|S|}E(S_v)
$$
$S_v$ 表示 $S$ 中属性 $A$ 取值为 $v$ 的实例子集，
$|S_v|$ 表示属性 $A$ 在 $S$ 中取值为 $v$ 的实例数，$|S|$ 表示 $S$ 中实例的总数。
$\frac{|S_v|}{|S|}$ 表示 $S$ 中属性 $A$ 取值为 $v$ 的实例数占 $S$ 中实例数的比例。

例如
|Wind|Play|
|:-:|:-:|
|Weak|No|
|Strong|No|
|Weak|Yes|
|Weak|Yes|
|Strong|No|
$$
\begin{align*}
v &\in \{Weak, Strong\}\\
S &= \{No, No, Yes, Yes, No\}\\
S_{Weak} &= \{No, Yes, Yes\}\\
S_{Strong} &= \{No, No\}\\
|S| &= 5\\
|S_{Weak}| &= 3\\
|S_{Strong}| &= 2\\
Gain(S, Wind) &= E(S) - \frac{3}{5}E(S_{Weak}) - \frac{2}{5}E(S_{Strong})\\
\end {align*}
$$

信息增益率
$$
Gain\_ratio(S,A) = \frac{Gain(S,A)}{SI(A)}\\
SI(A) =E(A_v)= -\sum_{v\in Values(A)}\frac{|A_v|}{|A|}\log_2\frac{|A_v|}{|A|}
$$
$SI(A)$是集合$A$关于属性$A$的熵
熵低的属性具有更高的信息增益率

ID3算法
```python

def ID3(D, A):
    if D中所有实例属于同一类C:
        return 单节点树T，标记为C
    if A = 空集:
        return 单节点树T，标记为D中实例数最大的类
    从A中选择最优划分属性a*
    for a*的每一个值a*:
        为T生成一个分支，令Dv表示D中在a*上取值为a*的实例子集
        if Dv为空:
            为分支生成一个叶节点，标记为D中实例数最大的类
        else:
            以ID3(Dv, A-{a*})为分支节点
    return T
```

## 离散化策略
### 等宽法
将属性的值域分为具有相同宽度的区间，每个区间对应一个取值。
### gain-based方法
将属性的值域分为具有最大信息增益的区间，每个区间对应一个取值。
```python
def gain_based_discretization(A,S):
    # A是属性，S是样本集
    # 1.计算A的值域
    # 2.计算A的值域的中点
    # 3.计算每个中点的信息增益
    # 4.选择信息增益最大的中点作为分割点
    # 5.将A的值域分为两个区间
    # 6.递归地对两个区间重复上述过程

    
```
```



